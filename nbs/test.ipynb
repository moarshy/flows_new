{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import yaml\n",
    "import logging\n",
    "import tiktoken\n",
    "from IPython.display import display, Markdown\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flows_new.components import PrintComponent, SquareComponent\n",
    "from flows_new.blocks import Block\n",
    "from flows_new.workflow_builder import WorkflowBuilder\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 - Serial block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 message: Hello, World!\n",
      "1 message: Hello, World!\n",
      "2 message: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "comp1 = PrintComponent(\"Hello, World!\")\n",
    "comp2 = PrintComponent(component_order=1, expects_input=True)\n",
    "comp3 = PrintComponent(component_order=2, expects_input=True)\n",
    "\n",
    "block = Block([comp1, comp2, comp3], type='serial')\n",
    "block.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(block.components[0].output)\n",
    "print(block.components[1].output)\n",
    "print(block.components[2].output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 - Parallel block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 10:55:11,295\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "comp1 = PrintComponent(\"Hello, World!\")\n",
    "comp2 = PrintComponent(message=\"Hello\", component_order=1)\n",
    "comp3 = PrintComponent(message=\"World\", component_order=2)\n",
    "block = Block([comp1, comp2, comp3], type='parallel')\n",
    "block.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print(block.components[0].output)\n",
    "print(block.components[1].output)\n",
    "print(block.components[2].output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 - workflow builder with yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 message: Hello, World!\n",
      "1 message: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "builder = WorkflowBuilder('test2.yaml')\n",
    "builder.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(builder.blocks[0].components[0].output)\n",
    "print(builder.blocks[0].components[1].output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4 - Square component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 message: 4\n",
      "1 message: 16\n",
      "2 message: 256\n"
     ]
    }
   ],
   "source": [
    "# Build a serial block\n",
    "comp1 = SquareComponent(number=2)\n",
    "comp2 = SquareComponent(expects_input=True, component_order=1)\n",
    "comp3 = SquareComponent(expects_input=True, component_order=2)\n",
    "block = Block([comp1, comp2, comp3], type='serial')\n",
    "block.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(block.components[0].output)\n",
    "print(block.components[1].output)\n",
    "print(block.components[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 message: 4\n",
      "1 message: 16\n",
      "2 message: 256\n"
     ]
    }
   ],
   "source": [
    "workflow = WorkflowBuilder('test_sq.yaml')\n",
    "workflow.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "9\n",
      "hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(workflow.blocks[1].components[0].output)\n",
    "print(workflow.blocks[1].components[1].output)\n",
    "print(workflow.blocks[1].components[2].output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop reader component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows_new.components import Component\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFReaderComponent(Component):\n",
    "    def __init__(self, filename, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filename = filename\n",
    "        self.loader = PyPDFLoader(file_path=self.filename)\n",
    "\n",
    "    def execute(self):\n",
    "        self.output = self.loader.load()\n",
    "\n",
    "comp1 = PDFReaderComponent(filename='./data/zhou2020.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkerComponent(Component):\n",
    "    def __init__(self, chunk_size=1500, chunk_overlap=100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "    def execute(self):\n",
    "        assert self.input_from_prev is not None, \"Component expects input but none was provided\"\n",
    "        self.output = self.splitter.split_documents(self.input_from_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIFAISSComponent(Component):\n",
    "    def __init__(self, save_path, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "\n",
    "    def execute(self):\n",
    "        # Assert self.input_from_prev is not None\n",
    "        assert self.input_from_prev is not None, \"Component expects input but none was provided\"\n",
    "\n",
    "        input = self.input_from_prev\n",
    "        store = FAISS.from_documents(input, embedding=OpenAIEmbeddings())\n",
    "        self.output = store\n",
    "\n",
    "        if self.save_path is not None:\n",
    "            faiss.write_index(store.index, f\"{self.save_path}/docs.index\")\n",
    "\n",
    "            store.index = None\n",
    "            \n",
    "            # Pickle and store the VecDB.\n",
    "            with open(f\"{self.save_path}/faiss_store.pkl\", \"wb\") as f:\n",
    "                pickle.dump(store, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1 = PDFReaderComponent(filename='./data/zhou2020.pdf')\n",
    "comp2 = ChunkerComponent(expects_input=True, component_order=1)\n",
    "comp3 = OpenAIFAISSComponent(save_path='./data', component_order=1, expects_input=True)\n",
    "\n",
    "block = Block([comp1, comp2, comp3], type='serial')\n",
    "block.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 11:59:46,872\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "workflow = WorkflowBuilder('read_pdf.yaml')\n",
    "workflow.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<flows_new.components.PDFReaderComponent at 0x15f0fbaf0>,\n",
       " <flows_new.components.ChunkerComponent at 0x1074e6950>,\n",
       " <flows_new.components.OpenAIFAISSComponent at 0x15f0f9d50>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.blocks[0].components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load retriever and chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRetrieversComponent(Component):\n",
    "    def __init__(self, path, k, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "        self.k = 4\n",
    "\n",
    "    def execute(self):\n",
    "        index = faiss.read_index(f\"{self.path}/docs.index\")\n",
    "\n",
    "        with open(f\"{self.path}/faiss_store.pkl\", \"rb\") as f:\n",
    "            vectorstore = pickle.load(f)\n",
    "\n",
    "        vectorstore.index = index\n",
    "\n",
    "        self.output = vectorstore.as_retriever(search_kwargs={'k': self.k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverComponent(Component):\n",
    "    def __init__(self, query, concatenate_docs, max_tokens=3000, enc_model=\"gpt-3.5-turbo\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.query = query\n",
    "        self.concatenate_docs = concatenate_docs\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        self.enc_model = enc_model\n",
    "        try:\n",
    "            self.encoding = tiktoken.encoding_for_model(self.enc_model)\n",
    "        except KeyError:\n",
    "            logger.info(f\"Encoding for model {self.enc_model} not found. Using default encoding.\")\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def execute(self):\n",
    "        assert self.input_from_prev is not None, \"Component expects input but none was provided\"\n",
    "\n",
    "        retriever = self.input_from_prev\n",
    "        docs = retriever.get_relevant_documents(self.query)\n",
    "\n",
    "        if self.concatenate_docs:\n",
    "            self.output = self.concatenate_documents(docs)\n",
    "\n",
    "    def concatenate_documents(self, documents):\n",
    "        \"\"\"Combine documents up to a certain token limit.\"\"\"\n",
    "        combined_docs = \"\"\n",
    "        token_count = 0\n",
    "        used_docs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            doc_tokens = self.calculate_tokens(doc.page_content)\n",
    "            if (token_count + doc_tokens) <= self.max_tokens:\n",
    "                combined_docs += f\"\\n\\n{doc.page_content}\\nSource: {doc.metadata['source']}\"\n",
    "                token_count += doc_tokens\n",
    "                used_docs.append(doc)\n",
    "\n",
    "        return combined_docs, used_docs\n",
    "    \n",
    "    def calculate_tokens(self, document):\n",
    "        \"\"\"Calculate the number of tokens in a list of documents.\"\"\"\n",
    "        return len(self.encoding.encode(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer_system_template = \"\"\"\n",
    "As an AI assistant helping answer a user's question, your task is to provide the answer to the user's question based on the collection of documents provided. Each document is demarcated by the 'Source:' tag. \n",
    "\n",
    "In most cases, the answer to the user's question can be found in one of the documents.\n",
    "\n",
    "If the documents do not contain the required information to answer user's question, respond with 'I don't know'. In this case, you can provide a link to the Chainlink documentation.\n",
    "\n",
    "Each point in your answer should be formatted with corresponding reference(s) using markdown. Conclude your response with a footnote that enumerates all the references involved. Please make sure to use only the references provided in the documents and not to use any external references. \n",
    "\n",
    "The footnote should be formatted as follows: \n",
    "```\n",
    "References:\n",
    "[^1^]: <reference 1> \n",
    "[^2^]: <reference 2> \n",
    "[^3^]: <reference 3>\n",
    "```\n",
    "Please avoid duplicating references. For example, if the same reference is used twice in the answer, please only include it once in the footnote.\n",
    "\"\"\"\n",
    "\n",
    "final_answer_human_template = \"\"\"\n",
    "User's question: {question}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QandAComponent(Component):\n",
    "    def __init__(self, query, system_message, user_message, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.query = query\n",
    "        self.system_message = system_message\n",
    "        self.user_message = user_message\n",
    "        FINAL_ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(self.system_message),\n",
    "                HumanMessagePromptTemplate.from_template(self.user_message),\n",
    "            ]\n",
    "        )\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.)\n",
    "        self.chain = LLMChain(llm=llm, prompt=FINAL_ANSWER_PROMPT)\n",
    "\n",
    "    def execute(self):\n",
    "        assert self.input_from_prev is not None, \"Component expects input but none was provided\"\n",
    "\n",
    "        relevant_docs = self.input_from_prev\n",
    "        self.output = self.chain.predict(question=self.query, document=relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1 = LoadRetrieversComponent(path='./data', k=4)\n",
    "query = \"What is this about?\"\n",
    "comp2 = RetrieverComponent(query=query, concatenate_docs=True, component_order=1, expects_input=True)\n",
    "comp3 = QandAComponent(query=query, system_message=final_answer_system_template, user_message=final_answer_human_template, component_order=2, expects_input=True)\n",
    "\n",
    "block = Block([comp1, comp2, comp3], type='serial')\n",
    "block.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This document discusses the research on multifunctional bioactive peptides (MBAPs), specifically focusing on antihypertensive peptides[^1^]. MBAPs have various properties, such as antitumor, antioxidant, and immunomodulating activities[^1^]. The document mentions that many achievements have been made in the field of MBAPs, particularly in terms of antihypertensive peptides[^1^]. However, the commercial products that make use of MBAPs are still very rare[^1^]. The document suggests that more in-depth studies, including in vivo testing and investigation of the underlying mechanism of action, should be the focus of future research[^1^].\n",
       "\n",
       "References:\n",
       "[^1^]: J. Zhou, et al. Food Research International 134 (2020) 109230"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(block.components[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 12:08:41,356\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "workflow = WorkflowBuilder('qanda.yaml')\n",
    "workflow.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The document is a review on mushroom-derived bioactive peptides, focusing on their preparation and biological activities[^1^]. It discusses the extraction of bioactive peptides directly or indirectly from mushrooms or their mycelia. Mushrooms are considered promising sources of bioactive peptides due to their high-quality proteins[^1^]. The review highlights the beneficial effects of mushroom bioactive peptides, including antihypertensive, antioxidant, and antimicrobial activities[^1^]. It also mentions that more in-depth studies, such as in vivo testing and investigation of the underlying mechanism of action, should be the focus of future research in this field[^1^].\n",
       "\n",
       "References:\n",
       "[^1^]: Juanjuan Zhou et al. \"A review on mushroom-derived bioactive peptides: Preparation and biological activities.\" Food Research International 134 (2020): 109230."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(workflow.blocks[0].components[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flows_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
